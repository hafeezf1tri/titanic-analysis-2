val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# ðŸ“Œ Step 7: Feature Importance
# Show which features contribute the most to predictions
importance(rf_model)
#varImpPlot(rf_model)  # Plot feature importance
# Extract feature importance and convert to dataframe
feature_importance <- as.data.frame(importance(rf_model))
# Verify column names
print(colnames(feature_importance))  # Check available column names
# Use the correct column name for sorting
feature_importance$Feature <- rownames(feature_importance)  # Add feature names
feature_importance <- feature_importance[order(-feature_importance$`MeanDecreaseGini`), ]  # Correct column reference
# ðŸ“Œ Step 1: Install Required Packages (if not already installed)
packages <- c("randomForest", "caret")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the necessary libraries
library(randomForest)  # For Random Forest Regression
library(caret)  # For dataset splitting and evaluation
# ðŸ“Œ Step 2: Load and Prepare the Dataset
# Load dataset (Ensure Titanic_processed.csv is in the working directory)
titanic_data <- read.csv("Titanic_processed.csv")
# Remove unnecessary column (Passenger No.)
titanic_data$No. <- NULL
# Convert categorical variable 'Sex' to factor (if not already encoded)
titanic_data$Sex <- as.factor(titanic_data$Sex)
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# ðŸ“Œ Step 3: Split Dataset into Train (80%), Test (10%), Validation (10%)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data[trainIndex, ]
temp_data <- titanic_data[-trainIndex, ]
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# ðŸ“Œ Step 4: Train a Random Forest Regression Model
set.seed(42)  # Ensure reproducibility
rf_model <- randomForest(Survived ~ ., data = train_data, ntree = 500, importance = TRUE)
# ðŸ“Œ Step 5: Evaluate Model Performance
# Predict on test set
rf_preds <- predict(rf_model, newdata = test_data)
# Convert continuous predictions to binary (0 or 1) for classification-like evaluation
rf_class_preds <- ifelse(rf_preds > 0.5, 1, 0)
# Calculate Mean Squared Error (MSE)
mse <- mean((rf_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - rf_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# ðŸ“Œ Step 6: Validate the Model
# Predict on validation set
val_preds <- predict(rf_model, newdata = validation_data)
# Convert to binary for evaluation
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# ðŸ“Œ Step 7: Feature Importance
# Show which features contribute the most to predictions
importance(rf_model)
#varImpPlot(rf_model)  # Plot feature importance
# Extract feature importance and convert to dataframe
feature_importance <- as.data.frame(importance(rf_model))
# Verify column names
print(colnames(feature_importance))  # Check available column names
# Use the correct column name for sorting
feature_importance$Feature <- rownames(feature_importance)  # Add feature names
feature_importance <- feature_importance[order(-feature_importance$MeanDecreaseGini), ]  # Correct column reference
# ðŸ“Œ Step 1: Install Required Packages (if not already installed)
packages <- c("randomForest", "caret")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the necessary libraries
library(randomForest)  # For Random Forest Regression
library(caret)  # For dataset splitting and evaluation
# ðŸ“Œ Step 2: Load and Prepare the Dataset
# Load dataset (Ensure Titanic_processed.csv is in the working directory)
titanic_data <- read.csv("Titanic_processed.csv")
# Remove unnecessary column (Passenger No.)
titanic_data$No. <- NULL
# Convert categorical variable 'Sex' to factor (if not already encoded)
titanic_data$Sex <- as.factor(titanic_data$Sex)
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# ðŸ“Œ Step 3: Split Dataset into Train (80%), Test (10%), Validation (10%)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data[trainIndex, ]
temp_data <- titanic_data[-trainIndex, ]
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# ðŸ“Œ Step 4: Train a Random Forest Regression Model
set.seed(42)  # Ensure reproducibility
rf_model <- randomForest(Survived ~ ., data = train_data, ntree = 500, importance = TRUE)
# ðŸ“Œ Step 5: Evaluate Model Performance
# Predict on test set
rf_preds <- predict(rf_model, newdata = test_data)
# Convert continuous predictions to binary (0 or 1) for classification-like evaluation
rf_class_preds <- ifelse(rf_preds > 0.5, 1, 0)
# Calculate Mean Squared Error (MSE)
mse <- mean((rf_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - rf_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# ðŸ“Œ Step 6: Validate the Model
# Predict on validation set
val_preds <- predict(rf_model, newdata = validation_data)
# Convert to binary for evaluation
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# ðŸ“Œ Step 7: Feature Importance
# Show which features contribute the most to predictions
importance(rf_model)
varImpPlot(rf_model)  # Plot feature importance
print(colnames(feature_importance))
# ðŸ“Œ Step 1: Install Required Packages (if not already installed)
packages <- c("randomForest", "caret")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the necessary libraries
library(randomForest)  # For Random Forest Regression
library(caret)  # For dataset splitting and evaluation
# ðŸ“Œ Step 2: Load and Prepare the Dataset
# Load dataset (Ensure Titanic_processed.csv is in the working directory)
titanic_data <- read.csv("Titanic_processed.csv")
# Remove unnecessary column (Passenger No.)
titanic_data$No. <- NULL
# Convert categorical variable 'Sex' to factor (if not already encoded)
titanic_data$Sex <- as.factor(titanic_data$Sex)
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# ðŸ“Œ Step 3: Split Dataset into Train (80%), Test (10%), Validation (10%)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data[trainIndex, ]
temp_data <- titanic_data[-trainIndex, ]
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# ðŸ“Œ Step 4: Train a Random Forest Regression Model
set.seed(42)  # Ensure reproducibility
rf_titanic_model <- randomForest(Survived ~ ., data = train_data, ntree = 500, importance = TRUE)
# ðŸ“Œ Step 5: Evaluate Model Performance
# Predict on test set
rf_preds <- predict(rf_titanic_model, newdata = test_data)
# Convert continuous predictions to binary (0 or 1) for classification-like evaluation
rf_class_preds <- ifelse(rf_preds > 0.5, 1, 0)
# Calculate Mean Squared Error (MSE)
mse <- mean((rf_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - rf_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# ðŸ“Œ Step 6: Validate the Model
# Predict on validation set
val_preds <- predict(rf_titanic_model, newdata = validation_data)
# Convert to binary for evaluation
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# ðŸ“Œ Step 7: Feature Importance
# Show which features contribute the most to predictions
importance(rf_titanic_model)
varImpPlot(rf_titanic_model)  # Plot feature importance
# ðŸ“Œ Step 1: Install Required Packages (if not already installed)
packages <- c("randomForest", "caret")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the necessary libraries
library(randomForest)  # For Random Forest Regression
library(caret)  # For dataset splitting and evaluation
# ðŸ“Œ Step 2: Load and Prepare the Dataset
# Load dataset (Ensure Titanic_processed.csv is in the working directory)
titanic_data <- read.csv("Titanic_processed.csv")
# Remove unnecessary column (Passenger No.)
titanic_data$No. <- NULL
# Convert categorical variable 'Sex' to factor (if not already encoded)
titanic_data$Sex <- as.factor(titanic_data$Sex)
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# ðŸ“Œ Step 3: Split Dataset into Train (80%), Test (10%), Validation (10%)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data[trainIndex, ]
temp_data <- titanic_data[-trainIndex, ]
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# ðŸ“Œ Step 4: Train a Random Forest Regression Model
set.seed(42)  # Ensure reproducibility
rf_titanic_model <- randomForest(Survived ~ ., data = train_data, ntree = 500, importance = TRUE)
# ðŸ“Œ Step 5: Evaluate Model Performance
# Predict on test set
rf_preds <- predict(rf_titanic_model, newdata = test_data)
# Convert continuous predictions to binary (0 or 1) for classification-like evaluation
rf_class_preds <- ifelse(rf_preds > 0.5, 1, 0)
# Calculate Mean Squared Error (MSE)
mse <- mean((rf_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - rf_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# ðŸ“Œ Step 6: Validate the Model
# Predict on validation set
val_preds <- predict(rf_titanic_model, newdata = validation_data)
# Convert to binary for evaluation
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# ðŸ“Œ Step 7: Feature Importance
# Show which features contribute the most to predictions
importance(rf_titanic_model)
#varImpPlot(rf_titanic_model)  # Plot feature importance
# Convert variable importance to a dataframe
feature_importance <- as.data.frame(importance(rf_titanic_model))
feature_importance$Feature <- rownames(feature_importance)
# Plot using ggplot2 (bar graph)
ggplot(feature_importance, aes(x = reorder(Feature, `%IncMSE`), y = `%IncMSE`)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance in Random Forest Model",
x = "Features",
y = "Mean Decrease in Accuracy")
# Define necessary packages
packages <- c("e1071", "caret", "ggplot2", "dplyr")
# Install missing packages
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the required libraries
library(e1071)  # For Support Vector Regression (SVR)
library(caret)   # For dataset splitting
library(ggplot2) # For visualization
library(dplyr)   # For data manipulation
# Load dataset (Ensure Titanic_Cleaned.csv is in the working directory)
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Convert categorical variables to numeric (Required for SVR)
titanic_data$Sex <- ifelse(titanic_data$Sex == "male", 1, 0)  # Male = 1, Female = 0
titanic_data$Pclass <- as.numeric(titanic_data$Pclass)  # Convert factor to numeric
# Remove unnecessary columns (if any)
titanic_data$No. <- NULL
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# Normalize features (SVR is sensitive to feature scaling)
X <- as.data.frame(scale(X))  # Standardize features
# Combine back into a dataset for splitting
titanic_data_scaled <- cbind(X, Survived = y)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data_scaled[trainIndex, ]
temp_data <- titanic_data_scaled[-trainIndex, ]  # Remaining 20%
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# Train an SVR model
set.seed(42)
svr_model <- svm(Survived ~ ., data = train_data, kernel = "radial", cost = 1, epsilon = 0.1)
# Print model summary
print(svr_model)
# Predict on the test set
svr_preds <- predict(svr_model, newdata = test_data)
# Calculate Mean Squared Error (MSE)
mse <- mean((svr_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - svr_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# Predict on validation set
val_preds <- predict(svr_model, newdata = validation_data)
# Convert predictions to binary (since Survived is 0 or 1)
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# Calculate correlation between each feature and predictions
importance_scores <- apply(X, 2, function(feature) cor(feature, svr_preds))
# Define necessary packages
packages <- c("e1071", "caret", "ggplot2", "dplyr")
# Install missing packages
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the required libraries
library(e1071)  # For Support Vector Regression (SVR)
library(caret)   # For dataset splitting
library(ggplot2) # For visualization
library(dplyr)   # For data manipulation
# Load dataset (Ensure Titanic_Cleaned.csv is in the working directory)
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Convert categorical variables to numeric (Required for SVR)
titanic_data$Sex <- ifelse(titanic_data$Sex == "male", 1, 0)  # Male = 1, Female = 0
titanic_data$Pclass <- as.numeric(titanic_data$Pclass)  # Convert factor to numeric
# Remove unnecessary columns (if any)
titanic_data$No. <- NULL
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# Normalize features (SVR is sensitive to feature scaling)
X <- as.data.frame(scale(X))  # Standardize features
# Combine back into a dataset for splitting
titanic_data_scaled <- cbind(X, Survived = y)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data_scaled[trainIndex, ]
temp_data <- titanic_data_scaled[-trainIndex, ]  # Remaining 20%
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# Train an SVR model
set.seed(42)
svr_model <- svm(Survived ~ ., data = train_data, kernel = "radial", cost = 1, epsilon = 0.1)
# Print model summary
print(svr_model)
# Predict on the test set
svr_preds <- predict(svr_model, newdata = test_data)
# Calculate Mean Squared Error (MSE)
mse <- mean((svr_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - svr_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# Predict on validation set
val_preds <- predict(svr_model, newdata = validation_data)
# Convert predictions to binary (since Survived is 0 or 1)
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# Calculate correlation between each feature and predictions
importance_scores <- apply(X, 2, function(feature) cor(feature, svr_preds))
# Define required packages
packages <- c("e1071", "caret", "ggplot2", "dplyr")
# Install missing packages
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the required libraries
library(e1071)  # Support Vector Regression
library(caret)   # Data splitting
library(ggplot2) # Visualization
library(dplyr)   # Data manipulation
# Load dataset (Ensure Titanic_Cleaned.csv is in the working directory)
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Convert categorical variables to numeric (Required for SVR)
titanic_data$Sex <- ifelse(titanic_data$Sex == "male", 1, 0)  # Male = 1, Female = 0
titanic_data$Pclass <- as.numeric(titanic_data$Pclass)  # Convert factor to numeric
# Remove unnecessary columns (if any)
titanic_data$No. <- NULL
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# Normalize features (SVR is sensitive to feature scaling)
X <- as.data.frame(scale(X))  # Standardize features
# Combine back into a dataset for splitting
titanic_data_scaled <- cbind(X, Survived = y)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data_scaled[trainIndex, ]
temp_data <- titanic_data_scaled[-trainIndex, ]  # Remaining 20%
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# Train an SVR model
set.seed(42)
svr_model <- svm(Survived ~ ., data = train_data, kernel = "radial", cost = 1, epsilon = 0.1)
# Print model summary
print(svr_model)
# Predict on the test set
svr_preds <- predict(svr_model, newdata = test_data)
# Calculate Mean Squared Error (MSE)
mse <- mean((svr_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - svr_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# Predict on validation set
val_preds <- predict(svr_model, newdata = validation_data)
# Convert predictions to binary (since Survived is 0 or 1)
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# Extract only the feature columns from test data
X_test <- test_data[, !(names(test_data) %in% c("Survived"))]
# Ensure dimensions match
print(dim(X_test))  # Check feature dimensions
print(length(svr_preds))  # Check prediction length
# Calculate correlation between each feature and SVR predictions
importance_scores <- apply(X_test, 2, function(feature) cor(feature, svr_preds, use = "complete.obs"))
# Convert to dataframe for visualization
feature_importance <- data.frame(Feature = names(importance_scores), Importance = abs(importance_scores))
# Sort and plot as a bar graph
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance in SVR Model",
x = "Features",
y = "Correlation with Predictions")
# Define required packages
packages <- c("e1071", "caret", "ggplot2", "dplyr")
# Install missing packages
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load the required libraries
library(e1071)  # Support Vector Regression
library(caret)   # Data splitting
library(ggplot2) # Visualization
library(dplyr)   # Data manipulation
# Load dataset (Ensure Titanic_Cleaned.csv is in the working directory)
titanic_data <- read.csv("Titanic_Cleaned.csv")
# Convert categorical variables to numeric (Required for SVR)
titanic_data$Sex <- ifelse(titanic_data$Sex == "male", 1, 0)  # Male = 1, Female = 0
titanic_data$Pclass <- as.numeric(titanic_data$Pclass)  # Convert factor to numeric
# Remove unnecessary columns (if any)
titanic_data$No. <- NULL
# Define dependent (y) and independent (X) variables
X <- titanic_data[, !(names(titanic_data) %in% c("Survived"))]  # Features
y <- titanic_data$Survived  # Target variable
# Normalize features (SVR is sensitive to feature scaling)
X <- as.data.frame(scale(X))  # Standardize features
# Combine back into a dataset for splitting
titanic_data_scaled <- cbind(X, Survived = y)
set.seed(42)  # For reproducibility
# Create train index (80%)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- titanic_data_scaled[trainIndex, ]
temp_data <- titanic_data_scaled[-trainIndex, ]  # Remaining 20%
# Split remaining 20% into test (10%) and validation (10%)
testIndex <- createDataPartition(temp_data$Survived, p = 0.5, list = FALSE)
test_data <- temp_data[testIndex, ]
validation_data <- temp_data[-testIndex, ]
# Train an SVR model
set.seed(42)
svr_model <- svm(Survived ~ ., data = train_data, kernel = "radial", cost = 1, epsilon = 0.1)
# Print model summary
print(svr_model)
# Predict on the test set
svr_preds <- predict(svr_model, newdata = test_data)
# Calculate Mean Squared Error (MSE)
mse <- mean((svr_preds - test_data$Survived)^2)
print(paste("Mean Squared Error:", mse))
# Calculate R-squared (coefficient of determination)
ss_total <- sum((test_data$Survived - mean(test_data$Survived))^2)
ss_residual <- sum((test_data$Survived - svr_preds)^2)
r2 <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r2))
# Predict on validation set
val_preds <- predict(svr_model, newdata = validation_data)
# Convert predictions to binary (since Survived is 0 or 1)
val_class_preds <- ifelse(val_preds > 0.5, 1, 0)
# Calculate validation accuracy
val_acc <- mean(val_class_preds == validation_data$Survived)
print(paste("Validation Accuracy:", val_acc))
# Extract only the feature columns from test data
X_test <- test_data[, !(names(test_data) %in% c("Survived"))]
# Ensure dimensions match
print(dim(X_test))  # Check feature dimensions
print(length(svr_preds))  # Check prediction length
# Calculate correlation between each feature and SVR predictions
importance_scores <- apply(X_test, 2, function(feature) cor(feature, svr_preds, use = "complete.obs"))
# Convert to dataframe for visualization
feature_importance <- data.frame(Feature = names(importance_scores), Importance = abs(importance_scores))
# Sort and plot as a bar graph
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "red") +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance in SVR Model",
x = "Features",
y = "Correlation with Predictions")
